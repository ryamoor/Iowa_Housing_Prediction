---
title: "Kaggle Housing Prices Prediction"
author: "Ryan Moore"
date: "September 23, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Instructions from Kaggle
*"Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence."*

*"With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home."*

<br>

###Aside from the Author
This data set is extremely interesting due to fact that most people's biggest purchase in their lives is a house. Who wouldn't want to know exactly what feaures matter and to what degree? Everyone assumes, if they have been in the real estate industry, that they can eye a house and price it based on experience. Some experts may be able to, but due to time constraints and the size of the housing market, scaling a predictive model is key. I chose this competition because it would be one of the best demonstrations of my skillset with a real life example. One of which that has a data science problem that is widely applicable to all businesses.

Data can be found here
<https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data>

My code can be found here
<https://github.com/Alaroc?tab=repositories>

<br>

### Table of Contents
* EDA
* Data Transformation / NA Handling
* Prediction/Validation

```{r, warning=FALSE, results="hide", message=FALSE}
#Loading necessary libraries
library(ggplot2)
library(data.table)
library(gridExtra)
library(dplyr)
library(corrplot)
library(randomForest)
library(xgboost)
library(leaps)
library(class)
library(moments)
library(RColorBrewer)


test<-fread("test.csv")
train<-fread("train.csv")
test[, ':='(SalePrice=NA, test_flag=1)]
train[, test_flag:=0]

all_data<-rbind(train, test)

```

## Exploratory Data Analysis

### Summary / Structure of the data
(The output from this code is very large and I've decided to exclude this.)

```{r, results='hide'}
str(all_data)
summary(all_data)
```

Most of the features are numeric but there are a few factor features.


### Number of NA's by Feature

```{r}
#creating a table that can be looped through to fix na values
na.Table<-data.table(colnames(train),sapply(all_data, function(y) sum(length(which(is.na(y))))))
#vector with features to be handled.
na.names<-na.Table[V2>0,]$V1

```
*Alley*, *PoolQC*, *Fence*, and *MiscFeatures* have a very large number of NA's. 

<br>

#### What is the distribution of the sale prices?
(Note that the colors are to be intrepreted as stacked and not overlaid)

#### How many houses were sold in each year?

```{r, warning=FALSE, message=FALSE}
ggplot(data=train, aes(x=SalePrice, fill=factor(YrSold)))+
        geom_histogram(position="stack")+
        scale_x_continuous(limits=c(50000,500000),labels=function(n){format(n, scientific = FALSE)})+ ylab("Houses Sold")


ggplot(data=train, aes(x=YrSold, fill=factor(YrSold)))+
        geom_bar()
```

It looks like 2010 hasn't ended by the time of the data collection or the test data is the end of 2010.
Let's test this.

```{r}
summary(train[YrSold==2010,]$MoSold)

```
Confirmed! The collection of the data stopped July 2010. 
Let's check if test follows the same pattern.

<br>

```{r}
ggplot(data=test, aes(x=YrSold, fill=factor(YrSold)))+
        geom_bar()
summary(test[YrSold==2010,]$MoSold)

```
It does!

<br>

### Have prices of houses increased over time in Iowa?
```{r,warning=FALSE, fig.width=14, fig.height=5}
ggplot(data=train, aes(x=MoSold, y=SalePrice, col=factor(YrSold)))+
        geom_point(alpha=.5)+
        geom_smooth()+
        xlab("Month Sold")+
        ylab("Sales Price")+
        scale_y_continuous(limits=c(50000,400000),labels=function(n){format(n, scientific = FALSE)})+
        scale_x_discrete(limits=1:12)+
        facet_grid(.~YrSold)
```
```{r,warning=FALSE,echo=FALSE,  fig.width=14}
ggplot(data=train, aes(x=as.factor(MoSold), y=SalePrice, col=factor(YrSold)))+
        geom_boxplot()+
        xlab("Month Sold")+
        ylab("Sales Price")+
        scale_y_continuous(limits=c(50000,400000),labels=function(n){format(n, scientific = FALSE)})+
        scale_x_discrete(limits=1:12)+
        facet_grid(.~YrSold)

```

```{r,warning=FALSE}
descriptive<-train[, list(mean=mean(SalePrice),
          min=min(SalePrice),
          lower=quantile(SalePrice, .25, na.rm=TRUE),
          median=quantile(SalePrice, .50, na.rm=TRUE),
          upper=quantile(SalePrice, .75, na.rm=TRUE),
          max=max(SalePrice)), by=YrSold]

descriptive<-descriptive[order(descriptive$YrSold)]
descriptive
```

Based on this, I would conclude, sale prices around the mean and median have dropped while high end prices dipped in 2008, recovering slightly after the housing crisis.

<br>

### What features correlate most with sale price?
This will be done only using numeric variables.

```{r, fig.width=14, fig.height=10}
setnames(train, c("1stFlrSF","2ndFlrSF"), c("FirstFlrSF","SecondFlrSF"))
numeric<-sapply(train, is.numeric)
#numeric defined by previous code.
cormatrix<-cor(x=train[, numeric,with=FALSE],use='complete')
par(mfrow=c(1,1))
corrplot(cormatrix,  method="color",mar=c(1,1,1,1),rect.col="black", tl.col="black")

topcor<-data.table(Features=rownames(cormatrix),cormatrix[,35])
setnames(topcor, names(topcor), c("Features","Correlation_with_Sale_Price"))
setorder(topcor, -Correlation_with_Sale_Price)
topcor

```

<br>

#### OverallQual

```{r, warning=FALSE}
ggplot(data=train, aes(x=OverallQual, y=SalePrice))+
        geom_point(col="orange",alpha=.2)+
        geom_smooth(col="blue")+
scale_y_continuous(limits=c(50000,400000),labels=function(n){format(n, scientific = FALSE)})
```

#### GrLivArea

```{r, warning=FALSE}
ggplot(data=train, aes(x=GrLivArea, y=SalePrice))+
        geom_point(col="orange",alpha=.2)+
        geom_smooth(col="blue")+
scale_y_continuous(limits=c(50000,400000),labels=function(n){format(n, scientific = FALSE)})

```

#### GarageCars

```{r, message=FALSE,warning=FALSE}
ggplot(data=train, aes(x=jitter(GarageCars), y=SalePrice))+
        geom_point(col="orange",alpha=.2)+
        stat_smooth(col="blue")+
        xlab("Size of Garage Car Capacity")+
scale_y_continuous(limits=c(50000,400000),labels=function(n){format(n, scientific = FALSE)})

```

<br>


### Various Other Continuous Features
```{r, echo=FALSE, warning=FALSE, fig.width=14, fig.height=10}
g1<-ggplot(data=train, aes(x=GarageArea, y=SalePrice))+
        geom_point(col="orange",alpha=.2)+
        geom_smooth(col="blue")+
scale_y_continuous(limits=c(50000,400000),labels=function(n){format(n, scientific = FALSE)})

g2<-ggplot(data=train, aes(x=TotalBsmtSF, y=SalePrice))+
        geom_point(col="orange",alpha=.2)+
        geom_smooth(col="blue")+
scale_y_continuous(limits=c(50000,400000),labels=function(n){format(n, scientific = FALSE)})+
scale_x_continuous(limits=c(0,4000))
g3<-ggplot(data=train, aes(x=FirstFlrSF, y=SalePrice))+
        geom_point(col="orange",alpha=.2)+
        geom_smooth(col="blue")+
scale_y_continuous(limits=c(50000,400000),labels=function(n){format(n, scientific = FALSE)})
g4<-ggplot(data=train, aes(x=jitter(FullBath), y=SalePrice))+
        geom_point(col="orange",alpha=.2)+
        geom_smooth(col="blue")+
scale_y_continuous(limits=c(50000,400000),labels=function(n){format(n, scientific = FALSE)})
g5<-ggplot(data=train, aes(x=TotRmsAbvGrd, y=SalePrice))+
        geom_point(col="orange",alpha=.2)+
        geom_smooth(col="blue")+
scale_y_continuous(limits=c(50000,400000),labels=function(n){format(n, scientific = FALSE)})
g6<-ggplot(data=train, aes(x=YearBuilt, y=SalePrice))+
        geom_point(col="orange",alpha=.2)+
        geom_smooth(col="blue")+
scale_y_continuous(limits=c(50000,400000),labels=function(n){format(n, scientific = FALSE)})

grid.arrange(g1,g2,g3,g4,g5, g6, nrow=2, ncol=3)
```

<br>

#### How do categorical features affect sale price?
In this section I'll be using all of the categorical features in a linear regression to determine which plots might be the most interesting.

```{r}
cat_vars<-sapply(train, is.character)
train_cat<-train[,cat_vars,with=FALSE]
##Full Summary
fits <- train[, list(MyFits = lapply(.SD[, cat_vars, with = F], function(x) summary(lm(SalePrice ~ x))))]

adjRSquared <- train[, list(MyFits = lapply(.SD[, cat_vars, with = F], function(x) summary(lm(SalePrice ~ x))$adj.r.squared))]

Rsqtable<-as.data.table(cbind(names(train[,cat_vars,with=F]),as.numeric(unlist(adjRSquared))))
setnames(Rsqtable, names(Rsqtable),c("cat_variable", "adjRsquared"))
Rsqtable[order(-adjRsquared)]

```
We can see from above that from the adjusted R squares that **Neighborhood**, **Exterior  Quality**, **Kitchen Quality**, **Basement Quality** are our top features for explaining variance in sale price. Let's graph them to visualize how each of these affect sale price.

<br>

#### Neighborhood

```{r, warning=FALSE, fig.width=10, fig.height=7}
ggplot(data=train, aes(x=as.factor(Neighborhood), y=SalePrice, col=Neighborhood))+
        geom_boxplot()+
        scale_y_continuous(limits=c(50000,760000),labels=function(n){format(n, scientific = FALSE)})+theme(axis.text.x = element_text(angle = 60, hjust = 1))+
        xlab("Neighborhood")
```

#### Exterior Quality

```{r, warning=FALSE, fig.width=10, fig.height=9}
ggplot(data=train, aes(x=as.factor(ExterQual), y=SalePrice, col=ExterQual))+
        geom_boxplot()+
        scale_y_continuous(limits=c(50000,760000),labels=function(n){format(n, scientific = FALSE)})+theme(axis.text.x = element_text(angle = 60, hjust = 1))+
        xlab("Exterior Quality")
table(train$ExterQual, useNA = "always")
table(test$ExterQual, useNA = "always")
```

* ExterQual: Evaluates the quality of the material on the exterior
    + Ex	Excellent
    + Gd	Good
    + TA	Average/Typical
    + Fa	Fair
    + Po	Poor

**Exterior quality** shows that there should be 5 options but it only seems that there are 4 within our train set. It's missing PO for Poor. Test also follows this pattern.


<br>

#### Kitchen Quality

```{r, warning=FALSE, fig.width=10, fig.height=7}
ggplot(data=train, aes(x=as.factor(KitchenQual), y=SalePrice, col=KitchenQual))+
        geom_boxplot()+
        scale_y_continuous(limits=c(50000,760000),labels=function(n){format(n, scientific = FALSE)})+theme(axis.text.x = element_text(angle = 60, hjust = 1))+
        xlab("Kitchen Quality")
table(train$KitchenQual, useNA = "always")
table(test$KitchenQual, useNA = "always")
```

* KitchenQual: Kitchen quality
    + Ex	Excellent
    + Gd	Good
    + TA	Average/Typical
    + Fa	Fair
    + Po	Poor

**Kitchen Quality** shows that there should be 5 options but it only seems that there are 4 within our train set. It's missing PO for Poor. Test seems to have 1 NA.


<br>

#### Basement Height

```{r, warning=FALSE, fig.width=10, fig.height=7}
ggplot(data=train,na.rm=FALSE, aes(x=as.factor(BsmtQual), y=SalePrice, col=BsmtQual))+
        geom_boxplot()+
        scale_y_continuous(limits=c(50000,760000),labels=function(n){format(n, scientific = FALSE)})+theme(axis.text.x = element_text(angle = 60, hjust = 1))+
         xlab("Basement Height")
table(test$BsmtQual, useNA = "always")
table(train$BsmtQual, useNA = "always")
```
* BsmtQual: Evaluates the height of the basement
    + Ex	(100+ inches)
    + Gd	Good (90-99 inches)
    + TA	Typical (80-89 inches)
    + Fa	Fair (70-79 inches)
    + Po	Poor (<70 inches

**Basement Height** shows that there should be 5 options but PO or Poor is missing. Test and Train have multiple NA's so this may be where Poor was allocated.

<br>

### Various other categorical features

```{r, warning=FALSE, fig.width=10, fig.height=10}
par(mfrow=c(3,2))
boxplot(SalePrice ~ as.factor(train$Alley), train, xlab = "Alley", ylab = "Sale Price")
boxplot(SalePrice ~ as.factor(train$GarageFinish), train, xlab = "GarageFinish", ylab = "Sale Price")
boxplot(SalePrice ~ as.factor(train$Foundation), train, xlab = "Foundation", ylab = "Sale Price")
boxplot(SalePrice ~ as.factor(train$GarageType), train, xlab = "GarageType", ylab = "Sale Price")
boxplot(SalePrice ~ as.factor(train$HeatingQC), train, xlab = "HeatingQC", ylab = "Sale Price")
boxplot(SalePrice ~ as.factor(train$BsmtFinType1), train, xlab = "BsmtFinType1", ylab = "Sale Price")
```

<br>

## Data Treatment
Data treatment will be relegated to assigning values to NA's and transforming skewed variables to more normal distributions.

### NA Features
Let's see what the distribution is of features with NA's. *I have hidden the results to save space.*
```{r,results="hide"}
#creating a function to show the count and percentage of a given feature.
tblFun <- function(x){
        tbl <- table(x, useNA="always")
        res <- cbind(tbl,round(prop.table(tbl)*100,2))
        colnames(res) <- c('Count','Percentage')
        res
}

tblFun(all_data$LotFrontage)
tblFun(all_data$Alley)
tblFun(all_data$MasVnrType)
tblFun(all_data$MasVnrArea)
tblFun(all_data$BsmtQual)
tblFun(all_data$BsmtCond)
tblFun(all_data$BsmtExposure)
tblFun(all_data$BsmtFinType1)
tblFun(all_data$BsmtFinType2)
tblFun(all_data$Electrical)
tblFun(all_data$FireplaceQu)
tblFun(all_data$GarageType)
tblFun(all_data$GarageYrBlt)
tblFun(all_data$GarageFinish)
tblFun(all_data$GarageQual)
tblFun(all_data$GarageCond)
tblFun(all_data$PoolQC)
tblFun(all_data$Fence)
tblFun(all_data$MiscFeature)
```

After looking at these features in the data dictionary you can conclude the majority have just designated NA to mean that this feature is absence from the home. This means that these can just be changed to none. However, *Electrical*, and *LotFrontage* look like they could use adjustment.


### Handling NA's
Let's assign 0 to NA's as the majority of the NA's are signifying None for the feature. LotFrontage will be an example of changing a feature to match its neighborhood's average.

```{r, warnings=FALSE}
all_data[all_data=="",]<-0
all_data[is.na(all_data),]<-0
all_data[Electrical=="None",]$Electrical<-"SBrkr"
all_data[]$LotFrontage<-as.numeric(all_data$LotFrontage)
all_data[, meanLotFrontage:=mean(LotFrontage,na.rm=T), by=Neighborhood]
all_data[,LotFrontage2:=ifelse(is.na(all_data$LotFrontage), meanLotFrontage,LotFrontage)]

drop<-c("meanLotFrontage","LotFrontage")
all_data[,(drop):=NULL]
rm(drop)

```

### Transforming Skewed Features
Transforming skewed features will lead to better predictions as linear models perform better on normal distributions.
```{r}
numeric<-sapply(all_data, is.numeric)
abs_skew<-abs(apply(all_data[,numeric,with=F],2,skewness))
abs_skew
abs_skew<-abs_skew[abs_skew>1]
name_skew<-names(abs_skew)
all_data.skew<-all_data[,name_skew,with=F]
all_data.skew.m <- log1p(as.matrix(all_data.skew))


#Add in the new columns and delete old
all_data.norm<-cbind(all_data,all_data.skew.m)
all_data.norm[,(name_skew):=NULL]

#Rename the features starting with numbers
setnames(all_data.norm, c("1stFlrSF","2ndFlrSF","3SsnPorch"), c("FirstFlrSF","SecondFlrSF","ThreeSsnPorch"))

#Change the character features to factor
upd.cols = sapply(all_data.norm, is.character)
all_data.norm[, names(all_data.norm)[upd.cols] := lapply(.SD, as.factor), .SDcols = upd.cols]

#GarageYrBlt relegated GarageYrBlt to the last decade or not
all_data.norm$GarageYrBlt<-as.numeric(all_data.norm$GarageYrBlt)
all_data.norm$GarageYrBlt<-ifelse(all_data.norm$GarageYrBlt>2000,1,0)

#Split test and train since transformation is complete
train.final<-all_data.norm[test_flag==0,]
test.final<-all_data.norm[test_flag==1,]

```


<br>

## Prediction
Let's now use linear regression, random forest and extreme gradient boosting to predict house prices with k-folds CV.

<br>


#### Linear Regression
```{r}
set.seed(10)
folds<-sample(rep(1:10, length=nrow(train.final)))
table(folds)
cv.lm.errors<-matrix(NA, nrow=10,ncol=2)


for(k in 1:10)
{
    lm.fit<-lm(SalePrice~OverallQual+Neighborhood+GrLivArea+GarageCars+BsmtFinSF1+YearBuilt+LotArea+OverallCond+MSZoning, data=train.final[folds!=k,])
    pred<-predict(lm.fit, train.final[folds==k,])
    cv.lm.errors[k,1]<-sqrt(mean((train.final$SalePrice[folds==k]-pred)^2, na.rm=T))
    cv.lm.errors[k,2]<-summary(lm.fit)$adj.r.squared
}

#RMSE of each fold
cv.lm.errors

#Average RMSE
mean(cv.lm.errors[,1])

#Average adj.R.Squared
mean(cv.lm.errors[,2])

#Distribution of RMSE by fold
plot(cv.lm.errors[,1], pch=19, ylab="RMSE", xlab="K-Fold")
```
```{r, warnings=FALSE}
#plotting one of the folds
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
plot(lm.fit)
```


#### Random Forest

Looking for the optimal value for mtry which determines how many variables each decision tree subsets from the pool of paramters.

```{r, warning=FALSE, eval=FALSE}
oob.err<-matrix(NA, nrow=10,ncol=16)
test.err<-matrix(NA, nrow=10,ncol=16)
colindex<-1
folds<-sample(rep(1:10, length=nrow(train.norm)))

#trying to find the correct tuning parameters
for(mtry in 10:25){
        for (k in 1:10){
        train.folds<-train.final[,.I[folds!=k]]
        train.val<-train.final[folds==k,]
        fit<-randomForest(SalePrice~.-GarageYrBlt-Id-binnum, data=train.final,subset=train.folds, mtry=mtry, ntree=300)
        oob.err[k,colindex]<-fit$mse[300]
        pred<-predict(fit,train.val)
        test.err[k,colindex]<-with(train.val,sqrt(mean((SalePrice-pred)^2,na.rm=T)))
        #cat(mtry,k," ")
        }
        colindex<-colindex+1
}
test.err2<-as.data.table(test.err)
g.test.err<-sapply(test.err2, mean)

oob.err2<-as.data.table(oob.err)
g.oob.err<-sapply(oob.err2, mean)

par(mfrow=c(1,2))
#Note that these are on different scales.
plot(1:11,y=g.test.err, ylab="RMSE",type="b", pch=19)
plot(1:11,y=g.oob.err, ylab="RMSE",type="b", pch=19)

par(mfrow=c(1,1))
```

Now we can attempt to opimize the value for ntree which determines how many trees are built with random forest.
```{r, eval=FALSE}
set.seed(10)
folds<-sample(rep(1:10, length=nrow(train.final)))
oob.err.kag<-matrix(NA, nrow=10,ncol=4)
test.err.kag<-matrix(NA, nrow=10,ncol=4)
colindex<-1

for(ntree in seq(400,700,by=100) )
{
 for (k in 1:10){
        train.folds<-train.norm[,.I[folds!=k]]
        train.val<-train.norm[folds==k,]
        fit<-randomForest(SalePrice~.-GarageYrBlt-Id-binnum, data=train.norm,subset=train.folds, mtry=30, ntree=ntree)
        oob.err.kag[k,colindex]<-fit$mse[300]
        pred<-predict(fit,train.val)
        test.err.kag[k,colindex]<-with(train.val,sqrt(mean((SalePrice-pred)^2,na.rm=T)))
        #cat(mtry,ntree," ")
 }
        colindex<-colindex+1
}

test.err.kag2<-as.data.table(test.err.kag)
g.test.err.kag<-sapply(test.err.kag2, mean)

oob.err.kag2<-as.data.table(oob.err.kag)
g.oob.err.kag<-sapply(oob.err.kag2, mean)

par(mfrow=c(1,2))
plot(1:4, g.test.err.kag, pch=19, col=brewer.pal(12,"Paired"), type="b", ylab="Mean Squared Error", xlab="Trees built(400:700)",xlim=c(1,4), main="Test.err.RMSE")
plot(1:4, g.oob.err.kag, pch=19, col=brewer.pal(12,"Paired"), type="b", ylab="Mean Squared Error", xlab="Trees built(400:700)",xlim=c(1,4),main="Out of Bag Error")
par(mfrow=c(1,1))
#Variable importance plot of the last random forest model
varImpPlot(fit)

```

Using What we learned from the tuning
(leaving us with RMSE .14287 on the leaderboard)

```{r
fit<-randomForest(SalePrice~., data=train.final, mtry=9, ntree=500)
pred<-predict(fit,test.final)

test.final[,SalePrice:=pred]
sub9_23_2016<-test.final[,.(Id,SalePrice)]
sub9_23_2016<-sub9_23_2016[,SalePrice:=expm1(SalePrice)]
write.csv(sub9_23_2016, "sub9_23_2016.csv",row.names=F)

```

#### Extreme Gradient Boosting (xgb)
```{r, results='hide'}

features=names(train.final)[!(names(train.final) %in% c('Id',"SalePrice"))] 

wltst=sample(nrow(train.final),146)  

dval<-xgb.DMatrix(data=data.matrix(train.final[wltst,features,with=FALSE]),label=data.matrix(train.final[wltst,SalePrice]),missing=NA)
watchlist<-list(dval=dval)

clf <- xgb.train(params=list(  objective="reg:linear", 
                               booster = "gbtree",
                               eta=0.008, 
                               max_depth=6, 
                               subsample=0.85,
                               colsample_bytree=1) ,
                 data = xgb.DMatrix(data=data.matrix(train.final[-wltst,features,with=FALSE]),
                                    label=data.matrix(train.final[-wltst,SalePrice]),missing=NA), 
                 nrounds = 5000, 
                 verbose = 1,
                 print_every_n=5,
                 early_stopping_rounds    = 10,
                 watchlist           = watchlist,
                 maximize            = FALSE,
                 eval_metric='rmse'
)
#These take a significant amount of time and I have commented them out.
#importance_matrix <- xgb.importance(features, model = clf)
#xgb.plot.importance(importance_matrix)

pred<-predict(clf,xgb.DMatrix(data.matrix(test.final[,features,with=FALSE]),missing=NA))
test.final[,SalePrice:=pred]
sub9_23_2016<-test.final[,.(Id,SalePrice)]
sub9_23_2016_xgb<-sub9_23_2016[,SalePrice:=expm1(SalePrice)]
write.csv(sub9_23_2016_xgb, "sub9_23_2016_xgb.csv",row.names=F)

```
<br>

### Final Words
This model yields a RMSE of .12631 on the leaderboard(test set) and a rank of 130(as of this being written). The next steps that would be needed to improve this score would be to extract more predictive power from the features. Feature engineering that focuses on aggregating features that correlate and creation of baseline prices based on neighborhood and square footage would be worth investigating. 


Thank you for reading!
